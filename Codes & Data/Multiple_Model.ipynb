{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOCK PREDICTION USING RNN LSTM\n"
     ]
    }
   ],
   "source": [
    "print('STOCK PREDICTION USING RNN LSTM')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import  Dropout\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from keras import regularizers\n",
    "\n",
    "import matplotlib.pyplot as pltb\n",
    "import matplotlib.ticker as mtick\n",
    "fmt = '$%.0f'\n",
    "tick = mtick.FormatStrFormatter(fmt)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "import os\n",
    "\n",
    "IMAGES_PATH=r\"C:\\Users\\Shaaf Abdullah\\Pictures\\Plots\"\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Shaaf Abdullah\\FYP CODES\\Stocks_Data\"\n",
    "directory=os.listdir(path)\n",
    "Train_MSE={}\n",
    "Test_MSE={}\n",
    "Epochs={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File AbbottLaboratoriesPakLtd_final.csv does not exist: 'AbbottLaboratoriesPakLtd_final.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-afa6c7e05bd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_final.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mdata_csv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_final.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mdata_csv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Close'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File AbbottLaboratoriesPakLtd_final.csv does not exist: 'AbbottLaboratoriesPakLtd_final.csv'"
     ]
    }
   ],
   "source": [
    "for file in directory:\n",
    "    if file.endswith('_final.csv'):\n",
    "        keras.backend.clear_session()\n",
    "        data_csv = pd.read_csv(file)\n",
    "        name=file.strip('_final.csv')\n",
    "        data_csv[['Close']].plot()\n",
    "        save_fig(name+'_closing_price_curve')\n",
    "        plt.clf()\n",
    "        percentage_of_data = 1.0\n",
    "        data_to_use = int(percentage_of_data*(len(data_csv)-1))\n",
    "\n",
    "         # 80% of data will be of training\n",
    "        train_end = int(data_to_use*0.8)\n",
    "\n",
    "        total_data = len(data_csv)\n",
    "        print(\"total_data:\", total_data)\n",
    "        start = total_data - data_to_use\n",
    "\n",
    "        # Currently doing prediction only for 1 step ahead\n",
    "        steps_to_predict = 1\n",
    "\n",
    "        #close, compund, neg, neu, pos, open, high, low, volume\n",
    "        # Order -> 8,1,2,3,4,5,6,7,9\n",
    "        yt = data_csv.iloc[start:total_data,8] #close\n",
    "        yt1 = data_csv.iloc[start:total_data,1] #compund\n",
    "        yt2 = data_csv.iloc[start:total_data,2] #neg\n",
    "        yt3 = data_csv.iloc[start:total_data,3] #neu\n",
    "        yt4 = data_csv.iloc[start:total_data,4] #pos\n",
    "        yt5 = data_csv.iloc[start:total_data,5] #open\n",
    "        yt6 = data_csv.iloc[start:total_data,6] #high\n",
    "        yt7 = data_csv.iloc[start:total_data,7] #low\n",
    "        vt = data_csv.iloc[start:total_data,9] #volume\n",
    "\n",
    "        print(\"yt head (close) :\")\n",
    "        print(yt.head())\n",
    "        #shift next day close and next day compund\n",
    "        yt_ = yt.shift(-1) #shifted close\n",
    "        yt1_ = yt1.shift(-1) #shifted compund\n",
    "\n",
    "       #taking only: close, next_close, compund, next_compund, volume, open, high, low\n",
    "        data = pd.concat([yt, yt_, yt1, yt1_, vt, yt5, yt6, yt7], axis=1)\n",
    "        data.columns = ['yt', 'yt_', 'yt1', 'yt1_','vt', 'yt5', 'yt6', 'yt7']\n",
    "\n",
    "        data = data.dropna()\n",
    "     \n",
    "        print(data[:10])\n",
    "        # target variable - closed price\n",
    "        # after shifting\n",
    "        y = data['yt_'] #next_close\n",
    "\n",
    "        # close, compund, next_compund, volume, open, high, low   \n",
    "        cols = ['yt', 'yt1', 'yt1_', 'vt', 'yt5', 'yt6', 'yt7']\n",
    "        x = data[cols]\n",
    "        scaler_x = preprocessing.MinMaxScaler (feature_range=(-1, 1))\n",
    "        x = np.array(x).reshape((len(x) ,len(cols)))\n",
    "        x = scaler_x.fit_transform(x)\n",
    "\n",
    "        scaler_y = preprocessing.MinMaxScaler (feature_range=(-1, 1))\n",
    "        y = np.array (y).reshape ((len( y), 1))\n",
    "        y = scaler_y.fit_transform (y)\n",
    "        X_train = x[0 : train_end,]\n",
    "        X_test = x[train_end+1 : len(x),]    \n",
    "        y_train = y[0 : train_end] \n",
    "        y_test = y[train_end+1 : len(y)]  \n",
    "\n",
    "        X_train = X_train.reshape (X_train. shape + (1,)) \n",
    "        X_test = X_test.reshape(X_test.shape + (1,))\n",
    "        batch_size = 32\n",
    "        if(data_csv.shape[0]<1000):\n",
    "            nb_epoch = 100\n",
    "        else:\n",
    "            nb_epoch = 150\n",
    "        neurons = 25\n",
    "        dropout = 0.1\n",
    "\n",
    "        seed = 2016\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        model = Sequential ()\n",
    "        model.add(LSTM(neurons, return_sequences=True, activation='tanh', inner_activation='hard_sigmoid', input_shape=(len(cols), 1)))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(LSTM(neurons, return_sequences=True,  activation='tanh'))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(LSTM(neurons, activation='tanh'))\n",
    "        model.add(Dropout(dropout))\n",
    "\n",
    "        model.add(Dense(activity_regularizer=regularizers.l1(0.00001), output_dim=1, activation='linear'))\n",
    "        model.add(Activation('tanh'))\n",
    "\n",
    "        print(model.summary())\n",
    "        model.compile(loss='mean_squared_error' , optimizer='RMSprop')\n",
    "        history=model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, validation_split=0.2)\n",
    "        score_train = model.evaluate(X_train, y_train, batch_size =1)\n",
    "        score_test = model.evaluate(X_test, y_test, batch_size =1)\n",
    "        \n",
    "        Train_MSE[name]=round( score_train ,4)\n",
    "        Test_MSE[name]=round( score_test ,4)\n",
    "        Epochs[name]=nb_epoch\n",
    "        \n",
    "        print(\"in train MSE = \", round( score_train ,4)) \n",
    "        print(\"in test MSE = \", round(score_test ,4))\n",
    "        model.save(name+\"_model.h5\")\n",
    "        model.save_weights(name+\"_weights.ckpt\")\n",
    "        print(\"Saved model to disk\")\n",
    "        pred = model.predict(X_test) \n",
    "        pred = scaler_y.inverse_transform(np.array(pred).reshape((len(pred), 1)))\n",
    "\n",
    "        prediction_data = pred[-1]     \n",
    "\n",
    "        print (\"Inputs: {}\".format(model.input_shape))\n",
    "        print (\"Outputs: {}\".format(model.output_shape))\n",
    "        print (\"Actual input: {}\".format(X_test.shape))\n",
    "        print (\"Actual output: {}\".format(y_test.shape))\n",
    "\n",
    "        print (\"prediction data:\")\n",
    "        print (prediction_data)\n",
    "\n",
    "        print (\"actual data\")\n",
    "        X_test = scaler_x.inverse_transform(np.array(X_test).reshape((len(X_test), len(cols))))\n",
    "        print (X_test)\n",
    "\n",
    "        #########################################################################\n",
    "        '''Plotting'''\n",
    "        plt.plot(pred, label=\"predictions\")\n",
    "\n",
    "        y_test = scaler_y.inverse_transform(np.array(y_test).reshape((len( y_test), 1)))\n",
    "        plt.plot([row[0] for row in y_test], label=\"actual\")\n",
    "\n",
    "        plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=2)\n",
    "\n",
    "        ax = plt.axes()\n",
    "        ax.yaxis.set_major_formatter(tick)\n",
    "        save_fig(name+\"_prediction_plot\")\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "        plt.grid(True)\n",
    "        plt.gca().set_ylim(0, 1)\n",
    "        save_fig(name+\"_curves_plot\")\n",
    "        plt.show()\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        save_fig(\"Engro_learning_curves_plot\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AbbottLaboratoriesPakLtd_final.csv\n",
      "AbdullahShahaGhaziSugerMillsLtd_final.csv\n",
      "AdamSugarMillsLimited_final.csv\n",
      "AGPLimited_final.csv\n",
      "AgriautoIndustriesLtd_final.csv\n",
      "AgritechLimited_final.csv\n",
      "AkzoNobelPakistanLimitedAkzoNobelPakistanLimited_final.csv\n",
      "AlAbbasSugarMillsLtd_final.csv\n",
      "AlGhaziTractorsLimited_final.csv\n",
      "AlliedBankLtd_final.csv\n",
      "AlShaheerCorporatio_final.csv\n",
      "ArchromaPakistanLimited_final.csv\n",
      "ArifHabibCorporationLtd_final.csv\n",
      "ArujIndustriesLimited_final.csv\n",
      "AskariBankLimited_final.csv\n",
      "AtlasBatteryLtd_final.csv\n",
      "ATLASHONDA_final.csv\n",
      "AtTahurLtd_final.csv\n",
      "AttockCementPakLtd_final.csv\n",
      "AttockPetroleumLimited_final.csv\n",
      "AzgardNineLimited_final.csv\n",
      "BabaFariedSugarMillsLtd_final.csv\n",
      "BankAlfalahLtd_final.csv\n",
      "BankAlHabibLtd_final.csv\n",
      "BankislamiPakistanLtd_final.csv\n",
      "BankOfPunjab_final.csv\n",
      "BawanyAirProductsLtd_final.csv\n",
      "BergerPaintsPakistanLtd_final.csv\n",
      "BestwayCementLimited_final.csv\n",
      "BifoIndustriesLtd_final.csv\n",
      "BilalFibresLtd_final.csv\n",
      "BleesedTextileLtd_final.csv\n",
      "BurshaneLPGPakistanLimited_final.csv\n",
      "BuxlyPaintsLtd_final.csv\n",
      "ChakwalSpinningMillsLimited_final.csv\n",
      "ChashmaSugarMillsLtd_final.csv\n",
      "CheratCementCoLtd_final.csv\n",
      "CloverPakistanLimited_final.csv\n",
      "ColgatePalmolivePakLtd_final.csv\n",
      "ColonyTextileMillsLtd_final.csv\n",
      "CrescentTextileMillsLtd_final.csv\n",
      "DandotCementCoLtd_final.csv\n",
      "DawoodLawrencepurLtd_final.csv\n",
      "DesconOxychemLtd_final.csv\n",
      "DewanCementLimited_final.csv\n",
      "DewanFarooqueSpinningMillsLtd_final.csv\n",
      "DewanTextileMillsLtd_final.csv\n",
      "DGKhanCementCoLtd_final.csv\n",
      "DMIndustriesLimited_final.csv\n",
      "DSIndustriesLtd_final.csv\n",
      "DYNEAPak_final.csv\n",
      "EngroCorporationLimited_final.csv\n",
      "EngroFertilizersLimited_final.csv\n",
      "EngroPolymerChemicalsLtd_final.csv\n",
      "ExidePakistanLtd_final.csv\n",
      "FaranSugarMillsLtd_final.csv\n",
      "FatimaFertilizerCoLtd_final.csv\n",
      "FaujiCementCoLtd_final.csv\n",
      "FaujiFertilizerBinQasimLtd_final.csv\n",
      "FaujiFertilizerCoLtd_final.csv\n",
      "FaujiFoodsLimited_final.csv\n",
      "FaysalBankLimited_final.csv\n",
      "FectoCementLtd_final.csv\n",
      "Feroze1888MillsLtd_final.csv\n",
      "FerozsonsLaboratoriesLtd_final.csv\n",
      "FlyingCementCompanyLtd_final.csv\n",
      "FrieslandcampinaEngroPakistanLtd_final.csv\n",
      "GadoonTextileMillsLtd_final.csv\n",
      "GeneralTyre&RubberCo_final.csv\n",
      "GeneralTyreRubberCo_final.csv\n",
      "GhandharaIndustriesLtd_final.csv\n",
      "GhaniAutomobileIndustrie_final.csv\n",
      "GhaniGlobalHoldingsLimited_final.csv\n",
      "GhaziFabricsInternationalLtd_final.csv\n",
      "GlaxoSmithKlineHealthcarePakLtd_final.csv\n",
      "GlaxoSmithKlinePakistanLtd_final.csv\n",
      "GulAhmedTextileMillsLtd_final.csv\n",
      "HabibBankLimited_final.csv\n",
      "HabibMetropolitnBankLimited_final.csv\n",
      "HabibSugarMillsLtd_final.csv\n",
      "HascolPetroleumLtd_final.csv\n",
      "HBLInvestmentFund_final.csv\n",
      "HighnoonLaboratoriesLtd_final.csv\n",
      "HinopakMotorsLtd_final.csv\n",
      "HiraTextileMillsLtd_final.csv\n",
      "HiTechLubrcantsLimited_final.csv\n",
      "HondaAtlasCars(Pakistan)LtdXD_final.csv\n",
      "HuseinSugarMillsLimited_final.csv\n",
      "IBLHealthCareLimited_final.csv\n",
      "ICIPakistanLimited_final.csv\n",
      "ImperialSugarLimited_final.csv\n",
      "IndusDyeingManufacturing_final.csv\n",
      "IndusMotorCompanyLtd_final.csv\n",
      "InterloopLimited_final.csv\n",
      "IttehadChemicalLtd_final.csv\n",
      "JDWSugarMillsLtd_final.csv\n",
      "JSBankLimited_final.csv\n",
      "KhairpurSugarMillsLtd_final.csv\n",
      "KhyberTobaccoCoLtd_final.csv\n",
      "KohatCementCoLtd_final.csv\n",
      "KohinoorIndustriesLtd_final.csv\n",
      "KohinoorMillsLtd_final.csv\n",
      "KohinoorSpinningMillsLtd_final.csv\n",
      "KohinoorTextileMillsLtd_final.csv\n",
      "LoadsLimited_final.csv\n",
      "LotteChemicalPakistanLtd_final.csv\n",
      "LuckyCementLimited_final.csv\n",
      "MapleLeafCementFactoryLtd_final.csv\n",
      "MaqboolTextileMillsLtd_final.csv\n",
      "MasoodTextileMillsLtd_final.csv\n",
      "MatcoFoodsLimited_final.csv\n",
      "MCBBankLimited_final.csv\n",
      "MeezanBankLimited_final.csv\n",
      "MehranSugarMillsLtdXD_final.csv\n",
      "MianTextileIndustriesLtd_final.csv\n",
      "MillatTractorsLtd_final.csv\n",
      "MirpurkhasSugarMillsLtd_final.csv\n",
      "MitchellsFruitFarmsLtd_final.csv\n",
      "MurreeBreweryCompanyLtd_final.csv\n",
      "NationalBankOfPakist_final.csv\n",
      "NationalFoodsLtd_final.csv\n",
      "NazirCottonMillsLtd_final.csv\n",
      "NimirIndustrialChemicalLtd_final.csv\n",
      "NimirResinsLimited_final.csv\n",
      "NishatChuniaLtd_final.csv\n",
      "NishatMillsLtd_final.csv\n",
      "NoonSugarMillsLtd_final.csv\n",
      "OtsukaPakistanLtd_final.csv\n",
      "PakElektronLtd_final.csv\n",
      "PakInternationalAirlineCorpLtd_final.csv\n",
      "PakistanCablesLtd_final.csv\n",
      "PakistanNationalShippingCo_final.csv\n",
      "PakistanOxygenLimited_final.csv\n",
      "PakistanStateOilCoLtd_final.csv\n",
      "PakSuzukiMotorCo.Ltd_final.csv\n",
      "PioneerCementLtd_final.csv\n",
      "PowercementLimited_final.csv\n",
      "PremiumTextileMillsLtd_final.csv\n",
      "QuettaTextileMillsLtd_final.csv\n",
      "QuiceFoodIndustriesLtd_final.csv\n",
      "RedcoTextileLtd_final.csv\n",
      "RubyTextileMillsLtd_final.csv\n",
      "SafeMixConcreteLtd_final.csv\n",
      "SaifTextileMillsLtd_final.csv\n",
      "SakrandSugarMillsLtd_final.csv\n",
      "SaminTextilesLtd_final.csv\n",
      "SanofiAventisPakistanLtd_final.csv\n",
      "SapphireTextileMillsLtd_final.csv\n",
      "SazgarEngineeringWork_final.csv\n",
      "ServiceFabricsLtd_final.csv\n",
      "ShadabTextileMillsLtd_final.csv\n",
      "ShahmuradSugarMillsLtd_final.csv\n",
      "ShahtajTextileLtd_final.csv\n",
      "ShakarganjLimited_final.csv\n",
      "ShellPakistanLtd_final.csv\n",
      "ShezanInternationalLtd_final.csv\n",
      "ShieldCorporationLtd_final.csv\n",
      "SiemensEngCoLtd_final.csv\n",
      "SilkBankLimited_final.csv\n",
      "SindhAbadgarsSugarMillsLtd_final.csv\n",
      "SitaraPeroxideLimited_final.csv\n",
      "SoneriBankLtd_final.csv\n",
      "StandardCharteredBankPakLtd_final.csv\n",
      "SuiNorthernGasPipeLineLtd_final.csv\n",
      "SuiSouthernGasCoLtd_final.csv\n",
      "SummitBankLimited_final.csv\n",
      "TandliawalaSugarMillsLtd_final.csv\n",
      "TataTextileMillsLtd_final.csv\n",
      "ThalLimited_final.csv\n",
      "ThattaCementCompanyLtd_final.csv\n",
      "TheBankofKhyber_final.csv\n",
      "TheSearleCompanyLtd_final.csv\n",
      "TreetCorporationLtd_final.csv\n",
      "TriStarMutualFundLtd_final.csv\n",
      "UnitedBankLtd_final.csv\n",
      "WahNobleChemicalsLtd_final.csv\n",
      "WAVESSingerPakistanLtd_final.csv\n",
      "WyethPakistanLimited_final.csv\n",
      "YousufWeavingMillsLimited_final.csv\n",
      "ZahidjeeTextileMillsLtd_final.csv\n",
      "ZephyrTextileLimited_final.csv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
